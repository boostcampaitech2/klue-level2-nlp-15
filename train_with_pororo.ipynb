{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed) :\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MODEL_NAME = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = pd.read_csv('/opt/ml/dataset/train/train_pororo.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='./results',          # output directory\n",
    "  save_total_limit=5,              # number of total save model.\n",
    "  save_steps=500,                   # model saving step.\n",
    "  num_train_epochs=2,              # total number of training epochs\n",
    "  learning_rate=5e-5,               # learning_rate\n",
    "  per_device_train_batch_size=32,  # batch size per device during training\n",
    "  per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  logging_dir='./logs',            # directory for storing logs\n",
    "  logging_steps=100,              # log saving step.\n",
    "  evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                              # `no`: No evaluation during training.\n",
    "                              # `steps`: Evaluate every `eval_steps`.\n",
    "                              # `epoch`: Evaluate every end of epoch.\n",
    "  eval_steps = 500,            # evaluation step.\n",
    "  load_best_model_at_end = True \n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# special_token Î∂àÎü¨Ïò§Í∏∞\n",
    "MODEL_NAME = 'klue/bert-base'\n",
    "\n",
    "special_token_list = []\n",
    "with open('./dataset/pororo_special_token.txt', 'r', encoding = 'UTF-8') as f :\n",
    "    for token in f :\n",
    "        special_token_list.append(token.split('\\n')[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "added_token_num = tokenizer.add_special_tokens({\"additional_special_tokens\":list(set(special_token_list))})\n",
    "print(added_token_num)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 30\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config = model_config)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(model.get_input_embeddings())\n",
    "model.resize_token_embeddings(tokenizer.vocab_size + added_token_num)\n",
    "print(model.get_input_embeddings())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Î≥∏ Ï†úÏ∂ú(max_length = 128)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "models = []\n",
    "stf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed_everything(42))\n",
    "for fold, (train_idx, dev_idx) in enumerate(stf.split(dataset, list(dataset['label']))) :\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 30\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config = model_config)\n",
    "    model.to(device)\n",
    "\n",
    "    # Ï∂îÍ∞ÄÌïú token Í∞úÏàòÎßåÌÅº token embedding size ÎäòÎ†§Ï£ºÍ∏∞\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size + added_token_num)\n",
    "\n",
    "    train_dataset = dataset.iloc[train_idx]\n",
    "    dev_dataset = dataset.iloc[dev_idx]\n",
    "\n",
    "    train_label = label_to_num(train_dataset['label'].values)\n",
    "    dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "    tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "    RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=RE_train_dataset,         # training dataset\n",
    "    eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics         # define metrics function\n",
    "    )\n",
    "    trainer.train()\n",
    "    models.append(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def makedirs(path) :\n",
    "    try :\n",
    "        os.makedirs(path)\n",
    "    except OSError :\n",
    "        if not os.path.isdir(path) :\n",
    "            raise\n",
    "\n",
    "for i, model in enumerate(models) :\n",
    "    makedirs(f'./best_model/sixth_try/fold_{i}/')\n",
    "    model.save_pretrained(f'./best_model/sixth_try/fold_{i}/')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_dataset = pd.read_csv('./dataset/test_pororo.csv')\n",
    "test_dataset['label'] = 100\n",
    "test_label = list(map(int, test_dataset['label'].values))\n",
    "tokenized_test = tokenized_dataset(test_dataset, tokenizer)\n",
    "test_id = test_dataset['id']\n",
    "Re_test_dataset = RE_Dataset(tokenized_test ,test_label) \n",
    "\n",
    "dataloader = DataLoader(Re_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "oof_pred = None\n",
    "for i in range(5) :\n",
    "    model_name = '/opt/ml/code/best_model/sixth_try/fold_{}'.format(i)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size + added_token_num)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    output_pred = []\n",
    "    for i, data in enumerate(tqdm(dataloader)) :\n",
    "        with torch.no_grad() :\n",
    "            outputs = model(\n",
    "                input_ids=data['input_ids'].to(device),\n",
    "                attention_mask=data['attention_mask'].to(device),\n",
    "                token_type_ids=data['token_type_ids'].to(device)\n",
    "                )\n",
    "        logits = outputs[0]\n",
    "        prob = F.softmax(logits, dim = -1).detach().cpu().numpy()\n",
    "        output_pred.append(prob)\n",
    "    final_prob = np.concatenate(output_pred, axis = 0)\n",
    "\n",
    "    if oof_pred is None :\n",
    "        oof_pred = final_prob / 5\n",
    "    else :\n",
    "        oof_pred += final_prob / 5\n",
    "\n",
    "result = np.argmax(oof_pred, axis = -1)\n",
    "pred_answer = num_to_label(result)\n",
    "output_prob = oof_pred.tolist()\n",
    "\n",
    "output = pd.DataFrame({'id':test_id,'pred_label':pred_answer,'probs':output_prob,})\n",
    "output.to_csv('./prediction/submission_seventh.csv', index=False) # ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú ÏôÑÏÑ±Îêú ÏòàÏ∏°Ìïú ÎùºÎ≤® csv ÌååÏùº ÌòïÌÉúÎ°ú Ï†ÄÏû•."
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}