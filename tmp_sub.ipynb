{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Define paths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\" define train data and test data path \"\"\"\r\n",
    "import os\r\n",
    "from glob import glob\r\n",
    "\r\n",
    "# root path\r\n",
    "ROOT_PATH = os.path.abspath(\".\")\r\n",
    "\r\n",
    "# directory paths\r\n",
    "ROOT_DATA_DIR = os.path.join(os.path.dirname(ROOT_PATH), 'dataset')\r\n",
    "TRAIN_DATA_DIR = os.path.join(ROOT_DATA_DIR, 'train')\r\n",
    "TEST_DATA_DIR = os.path.join(ROOT_DATA_DIR, 'test')\r\n",
    "PRIDICTION_DIR = os.path.join(os.path.dirname(ROOT_PATH), 'prediction')\r\n",
    "BASELINE_DIR = os.path.join(os.path.dirname(ROOT_PATH), 'code')\r\n",
    "\r\n",
    "# files paths\r\n",
    "TRAIN_FILE_PATH = glob(os.path.join(TRAIN_DATA_DIR, '*'))[0]\r\n",
    "TEST_FILE_PATH = glob(os.path.join(TEST_DATA_DIR, '*'))[0]\r\n",
    "SAMPLE_SUBMISSION_PATH = os.path.join(PRIDICTION_DIR, 'sample_submission.csv')\r\n",
    "PORORO_TRAIN_PATH = os.path.join(TRAIN_DATA_DIR, 'pororo_train_typed_entity_marker_punct.csv')\r\n",
    "PORORO_TEST_PATH = os.path.join(TEST_DATA_DIR, 'pororo_test_typed_entity_marker_punct.csv')\r\n",
    "\r\n",
    "print(TRAIN_FILE_PATH, TEST_FILE_PATH,SAMPLE_SUBMISSION_PATH, PORORO_TRAIN_PATH, PORORO_TEST_PATH)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e79733201e78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# files paths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mTRAIN_FILE_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_DATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mTEST_FILE_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_DATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mSAMPLE_SUBMISSION_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPRIDICTION_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# set config"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
<<<<<<< HEAD
    "\"\"\" Set configuration as dictionary format \"\"\"\n",
    "\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "# login wandb and get today's date until hour and minute\n",
    "wandb.login()\n",
    "today = datetime.now().strftime(\"%m%d_%H:%M\")\n",
    "\n",
    "# Debug set to true in order to debug high-layer code.\n",
    "# CFG Configuration\n",
    "CFG = wandb.config # wandb.config provides functionality of easydict.EasyDict\n",
    "CFG.DEBUG = False\n",
    "\n",
    "# Dataset Config as constants\n",
    "CFG.num_labels = 30\n",
    "CFG.num_workers = 4\n",
    "CFG.split_ratio = 0 # not going to use validation/test set\n",
    "CFG.batch_size = 32\n",
    "\n",
    "# Train configuration\n",
    "CFG.user_name = \"jjonhwa\"\n",
    "CFG.file_base_name = f\"{CFG.user_name}_{today}\"\n",
    "# CFG.model_name = \"klue/roberta-base\" # https://huggingface.co/klue/roberta-base\n",
    "# CFG.model_name = \"monologg/koelectra-base-v3-discriminator\" # https://huggingface.co/monologg/koelectra-base-v3-discriminator\n",
    "# CFG.model_name = \"klue/roberta-large\"\n",
    "CFG.model_name = \"klue/roberta-large\"\n",
    "CFG.num_folds = 5 # 5 Fold as default\n",
    "CFG.num_epochs = 3 # \n",
    "CFG.max_token_length = 128 + 10 # refer to EDA where Q3 is 119, there are only 460 sentences out of 32k train set\n",
    "CFG.stopwords = []\n",
    "CFG.learning_rate = 5e-5\n",
    "CFG.weight_decay = 1e-2 # https://paperswithcode.com/method/weight-decay\n",
    "CFG.input_size = 1024\n",
    "CFG.output_size = 1024\n",
    "CFG.num_rnn_layers = 3\n",
    "CFG.dropout_rate = 0.0\n",
    "\n",
    "# training steps configurations\n",
    "CFG.save_steps = 500\n",
    "CFG.early_stopping_patience = 5\n",
    "CFG.warmup_steps = 500\n",
    "CFG.logging_steps = 100\n",
    "CFG.evaluation_strategy = 'epoch'\n",
    "CFG.evaluation_steps = 500\n",
    "\n",
    "# Directory configuration\n",
    "CFG.result_dir = os.path.join(os.path.dirname(ROOT_PATH), \"results\")\n",
    "CFG.saved_model_dir = os.path.join(os.path.dirname(ROOT_PATH), \"best_models\")\n",
    "CFG.logging_dir = os.path.join(os.path.dirname(ROOT_PATH), \"logs\")\n",
    "CFG.baseline_dir = os.path.join(os.path.dirname(ROOT_PATH), 'baseline-code')\n",
    "\n",
    "# file configuration\n",
    "CFG.result_file = os.path.join(CFG.result_dir, f\"{CFG.file_base_name}.csv\")\n",
    "CFG.saved_model_file = os.path.join(CFG.saved_model_dir, f\"{CFG.file_base_name}.pth\")\n",
    "CFG.logging_file = os.path.join(CFG.logging_dir, f\"{CFG.file_base_name}.log\")\n",
    "CFG.label_to_num_file = os.path.join(CFG.baseline_dir, 'dict_label_to_num.pkl')\n",
    "CFG.num_to_label_file = os.path.join(CFG.baseline_dir, \"dict_num_to_label.pkl\")\n",
    "CFG.train_file_path = TRAIN_FILE_PATH\n",
    "CFG.test_file_path = TEST_FILE_PATH\n",
    "CFG.sample_submission_file_path = SAMPLE_SUBMISSION_PATH\n",
    "\n",
    "# Other configurations\n",
=======
    "\"\"\" Set configuration as dictionary format \"\"\"\r\n",
    "\r\n",
    "import wandb\r\n",
    "from datetime import datetime\r\n",
    "from easydict import EasyDict\r\n",
    "\r\n",
    "# login wandb and get today's date until hour and minute\r\n",
    "wandb.login()\r\n",
    "today = datetime.now().strftime(\"%m%d_%H:%M\")\r\n",
    "\r\n",
    "# Debug set to true in order to debug high-layer code.\r\n",
    "# CFG Configuration\r\n",
    "CFG = wandb.config # wandb.config provides functionality of easydict.EasyDict\r\n",
    "CFG.DEBUG = False\r\n",
    "\r\n",
    "# Dataset Config as constants\r\n",
    "CFG.num_labels = 30\r\n",
    "CFG.num_workers = 4\r\n",
    "CFG.split_ratio = 0 # not going to use validation/test set\r\n",
    "CFG.batch_size = 32\r\n",
    "\r\n",
    "# Train configuration\r\n",
    "CFG.user_name = \"kyeonj\"\r\n",
    "CFG.file_base_name = f\"{CFG.user_name}_{today}\"\r\n",
    "# CFG.model_name = \"klue/roberta-base\" # https://huggingface.co/klue/roberta-base\r\n",
    "# CFG.model_name = \"monologg/koelectra-base-v3-discriminator\" # https://huggingface.co/monologg/koelectra-base-v3-discriminator\r\n",
    "# CFG.model_name = \"klue/roberta-large\"\r\n",
    "CFG.model_name = \"klue/roberta-large\"\r\n",
    "CFG.num_folds = 5 # 5 Fold as default\r\n",
    "CFG.num_epochs = 3 # \r\n",
    "CFG.max_token_length = 128+10 # refer to EDA where Q3 is 119, there are only 460 sentences out of 32k train set\r\n",
    "CFG.stopwords = []\r\n",
    "CFG.learning_rate = 5e-5\r\n",
    "CFG.weight_decay = 1e-2 # https://paperswithcode.com/method/weight-decay\r\n",
    "CFG.input_size = 1024\r\n",
    "CFG.output_size = 1024\r\n",
    "CFG.num_rnn_layers = 3\r\n",
    "CFG.dropout_rate = 0.0\r\n",
    "\r\n",
    "# training steps configurations\r\n",
    "CFG.save_steps = 500\r\n",
    "CFG.early_stopping_patience = 5\r\n",
    "CFG.warmup_steps = 500\r\n",
    "CFG.logging_steps = 100\r\n",
    "CFG.evaluation_strategy = 'epoch'\r\n",
    "CFG.evaluation_steps = 500\r\n",
    "\r\n",
    "# Directory configuration\r\n",
    "CFG.result_dir = os.path.join(os.path.dirname(ROOT_PATH), \"results\")\r\n",
    "CFG.saved_model_dir = os.path.join(os.path.dirname(ROOT_PATH), \"best_models\")\r\n",
    "CFG.logging_dir = os.path.join(os.path.dirname(ROOT_PATH), \"logs\")\r\n",
    "CFG.baseline_dir = os.path.join(os.path.dirname(ROOT_PATH), 'code')\r\n",
    "\r\n",
    "# file configuration\r\n",
    "CFG.result_file = os.path.join(CFG.result_dir, f\"{CFG.file_base_name}.csv\")\r\n",
    "CFG.saved_model_file = os.path.join(CFG.saved_model_dir, f\"{CFG.file_base_name}.pth\")\r\n",
    "CFG.logging_file = os.path.join(CFG.logging_dir, f\"{CFG.file_base_name}.log\")\r\n",
    "CFG.label_to_num_file = os.path.join(CFG.baseline_dir, 'dict_label_to_num.pkl')\r\n",
    "CFG.num_to_label_file = os.path.join(CFG.baseline_dir, \"dict_num_to_label.pkl\")\r\n",
    "CFG.train_file_path = TRAIN_FILE_PATH\r\n",
    "CFG.test_file_path = TEST_FILE_PATH\r\n",
    "CFG.sample_submission_file_path = SAMPLE_SUBMISSION_PATH\r\n",
    "\r\n",
    "# Other configurations\r\n",
>>>>>>> cfe8d98ad3af3878dc1e0f40692011813d5e17b3
    "CFG.load_best_model_at_end = True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "eng_to_kor = {\r\n",
    "    'quantity' : '물량'\r\n",
    "    ,'person' : '사람'\r\n",
    "    ,'term' : '임기'\r\n",
    "    ,'o' : '없음'\r\n",
    "    ,'event' : '이벤트'\r\n",
    "    ,'study_field':'스터디필드'\r\n",
    "    ,'material':'재료'\r\n",
    "    ,'city':'도시'\r\n",
    "    ,'time':'시간'\r\n",
    "    ,'animal':'동물'\r\n",
    "    ,'location':'위치'\r\n",
    "    ,'disease': '질병'\r\n",
    "    ,'civilization':'문명'\r\n",
    "    ,'occupation':'직업'\r\n",
    "    ,'organization':'조직'\r\n",
    "    ,'country':'나라'\r\n",
    "    ,'artifact':'유물'\r\n",
    "    ,'date':'날짜'\r\n",
    "    ,'plant':'식물'\r\n",
    "    ,'theory':'이론'\r\n",
    "}\r\n",
    "\r\n",
    "CFG.special_token_list = []\r\n",
    "for v in eng_to_kor.values():\r\n",
    "    CFG.special_token_list.append(\"*\"+v+\"*\")\r\n",
    "    CFG.special_token_list.append(\"^\"+v+\"^\")   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from torch.utils.data import Dataset\r\n",
    "import torch\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\r\n",
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\r\n",
    "from sklearn.model_selection import StratifiedKFold"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-07 04:05:59.775977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import random\r\n",
    "\r\n",
    "def seed_everything(seed) :\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    torch.cuda.manual_seed(seed)\r\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\r\n",
    "    torch.backends.cudnn.deterministic = True\r\n",
    "    torch.backends.cudnn.benchmark = False\r\n",
    "    np.random.seed(seed)\r\n",
    "    random.seed(seed)\r\n",
    "seed_everything(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
<<<<<<< HEAD
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def pull_out_dictionary(df_input: pd.DataFrame):\n",
    "  \"\"\" pull out str `{}` values from the pandas dataframe and shape it as a new column\"\"\"\n",
    "\n",
    "  df = df_input.copy()\n",
    "\n",
    "  # assign subject_entity and object_entity column values type as dictionary\n",
    "  df['subject_entity'] = df['subject_entity'].apply(lambda x: eval(x))\n",
    "  df['object_entity'] = df['object_entity'].apply(lambda x: eval(x))\n",
    "\n",
    "  # parse item inside of subject_entity and object_entity's dictionary values as columns of dataframe\n",
    "  # word, start_idx, end_idx, type as new columns \n",
    "  df = df.assign(\n",
    "      # subject_entity\n",
    "      subject_word=lambda x: x['subject_entity'].apply(lambda x: x['word']),\n",
    "      subject_start_idx=lambda x: x['subject_entity'].apply(lambda x: x['start_idx']),\n",
    "      subject_end_idx=lambda x: x['subject_entity'].apply(lambda x: x['end_idx']),\n",
    "      subject_type=lambda x: x['subject_entity'].apply(lambda x: x['type']),\n",
    "      \n",
    "      # object_entity\n",
    "      object_word=lambda x: x['object_entity'].apply(lambda x: x['word']),\n",
    "      object_start_idx=lambda x: x['object_entity'].apply(lambda x: x['start_idx']),\n",
    "      object_end_idx=lambda x: x['object_entity'].apply(lambda x: x['end_idx']),\n",
    "      object_type=lambda x: x['object_entity'].apply(lambda x: x['type']),\n",
    "  )\n",
    "\n",
    "  # drop subject_entity and object_entity column\n",
    "  df = df.drop(['subject_entity', 'object_entity'], axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "def preprocessing_dataset(dataset):\n",
    "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "  \n",
    "  dataset = pull_out_dictionary(dataset)\n",
    "\n",
    "  # rename columns subject_word as subject_entity, object_word as object_entity\n",
    "  dataset = dataset.rename(columns={'subject_word': 'subject_entity', 'object_word': 'object_entity'})\n",
    "\n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':dataset['subject_entity'],'object_entity':dataset['object_entity'],'label':dataset['label'],})\n",
    "  display(out_dataset.head(2))\n",
    "  return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\n",
    "  dataset = preprocessing_dataset(pd_dataset)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer, is_training:bool = True):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding= 'max_length',\n",
    "      # padding= True,\n",
    "      truncation=is_training,\n",
    "      max_length=CFG.max_token_length,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      )\n",
    "\n",
    "  # tokenized_sentences = tokenizer(\n",
    "  #         concat_entity,\n",
    "  #         list(dataset[\"sentence\"]),\n",
    "  #         return_tensors=\"pt\",\n",
    "  #         padding=True,\n",
    "  #         truncation=True,\n",
    "  #         max_length=256,\n",
    "  #         add_special_tokens=True,\n",
    "  #         # return_token_type_ids=False,\n",
    "  #     )\n",
    "  return tokenized_sentences\n"
=======
    "import pickle as pickle\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "\r\n",
    "def pull_out_dictionary(df_input: pd.DataFrame):\r\n",
    "  \"\"\" pull out str `{}` values from the pandas dataframe and shape it as a new column\"\"\"\r\n",
    "\r\n",
    "  df = df_input.copy()\r\n",
    "\r\n",
    "  # assign subject_entity and object_entity column values type as dictionary\r\n",
    "  df['subject_entity'] = df['subject_entity'].apply(lambda x: eval(x))\r\n",
    "  df['object_entity'] = df['object_entity'].apply(lambda x: eval(x))\r\n",
    "\r\n",
    "  # parse item inside of subject_entity and object_entity's dictionary values as columns of dataframe\r\n",
    "  # word, start_idx, end_idx, type as new columns \r\n",
    "  df = df.assign(\r\n",
    "      # subject_entity\r\n",
    "      subject_word=lambda x: x['subject_entity'].apply(lambda x: x['word']),\r\n",
    "      subject_start_idx=lambda x: x['subject_entity'].apply(lambda x: x['start_idx']),\r\n",
    "      subject_end_idx=lambda x: x['subject_entity'].apply(lambda x: x['end_idx']),\r\n",
    "      subject_type=lambda x: x['subject_entity'].apply(lambda x: x['type']),\r\n",
    "      \r\n",
    "      # object_entity\r\n",
    "      object_word=lambda x: x['object_entity'].apply(lambda x: x['word']),\r\n",
    "      object_start_idx=lambda x: x['object_entity'].apply(lambda x: x['start_idx']),\r\n",
    "      object_end_idx=lambda x: x['object_entity'].apply(lambda x: x['end_idx']),\r\n",
    "      object_type=lambda x: x['object_entity'].apply(lambda x: x['type']),\r\n",
    "  )\r\n",
    "\r\n",
    "  # drop subject_entity and object_entity column\r\n",
    "  df = df.drop(['subject_entity', 'object_entity'], axis=1)\r\n",
    "\r\n",
    "  return df\r\n",
    "\r\n",
    "class RE_Dataset(torch.utils.data.Dataset):\r\n",
    "  \"\"\" Dataset 구성을 위한 class.\"\"\"\r\n",
    "  def __init__(self, pair_dataset, labels):\r\n",
    "    self.pair_dataset = pair_dataset\r\n",
    "    self.labels = labels\r\n",
    "\r\n",
    "  def __getitem__(self, idx):\r\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\r\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\r\n",
    "    return item\r\n",
    "\r\n",
    "  def __len__(self):\r\n",
    "    return len(self.labels)\r\n",
    "\r\n",
    "def preprocessing_dataset(dataset):\r\n",
    "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\r\n",
    "  \r\n",
    "  dataset = pull_out_dictionary(dataset)\r\n",
    "\r\n",
    "  # rename columns subject_word as subject_entity, object_word as object_entity\r\n",
    "  dataset = dataset.rename(columns={'subject_word': 'subject_entity', 'object_word': 'object_entity'})\r\n",
    "\r\n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':dataset['subject_entity'],'object_entity':dataset['object_entity'],'label':dataset['label'],})\r\n",
    "  display(out_dataset.head(2))\r\n",
    "  return out_dataset\r\n",
    "\r\n",
    "def load_data(dataset_dir):\r\n",
    "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\r\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\r\n",
    "  dataset = preprocessing_dataset(pd_dataset)\r\n",
    "  \r\n",
    "  return dataset\r\n",
    "\r\n",
    "def tokenized_dataset(dataset, tokenizer):\r\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\r\n",
    "  concat_entity = []\r\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\r\n",
    "    temp = ''\r\n",
    "    temp = e01 + tokenizer.sep_token + e02\r\n",
    "    concat_entity.append(temp)\r\n",
    "  tokenized_sentences = tokenizer(\r\n",
    "      concat_entity,\r\n",
    "      list(dataset['sentence']),\r\n",
    "      return_tensors=\"pt\",\r\n",
    "      # padding= 'max_length',\r\n",
    "      padding= True,\r\n",
    "      truncation=True,\r\n",
    "      max_length=CFG.max_token_length,\r\n",
    "      add_special_tokens=True,\r\n",
    "      return_token_type_ids=False,\r\n",
    "      )\r\n",
    "\r\n",
    "  # tokenized_sentences = tokenizer(\r\n",
    "  #         concat_entity,\r\n",
    "  #         list(dataset[\"sentence\"]),\r\n",
    "  #         return_tensors=\"pt\",\r\n",
    "  #         padding=True,\r\n",
    "  #         truncation=True,\r\n",
    "  #         max_length=256,\r\n",
    "  #         add_special_tokens=True,\r\n",
    "  #         # return_token_type_ids=False,\r\n",
    "  #     )\r\n",
    "  return tokenized_sentences\r\n"
>>>>>>> cfe8d98ad3af3878dc1e0f40692011813d5e17b3
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "import sklearn\r\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\r\n",
    "\r\n",
    "def klue_re_micro_f1(preds, labels):\r\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\r\n",
    "    label_list = [\r\n",
    "        \"no_relation\",\r\n",
    "        \"org:top_members/employees\",\r\n",
    "        \"org:members\",\r\n",
    "        \"org:product\",\r\n",
    "        \"per:title\",\r\n",
    "        \"org:alternate_names\",\r\n",
    "        \"per:employee_of\",\r\n",
    "        \"org:place_of_headquarters\",\r\n",
    "        \"per:product\",\r\n",
    "        \"org:number_of_employees/members\",\r\n",
    "        \"per:children\",\r\n",
    "        \"per:place_of_residence\",\r\n",
    "        \"per:alternate_names\",\r\n",
    "        \"per:other_family\",\r\n",
    "        \"per:colleagues\",\r\n",
    "        \"per:origin\",\r\n",
    "        \"per:siblings\",\r\n",
    "        \"per:spouse\",\r\n",
    "        \"org:founded\",\r\n",
    "        \"org:political/religious_affiliation\",\r\n",
    "        \"org:member_of\",\r\n",
    "        \"per:parents\",\r\n",
    "        \"org:dissolved\",\r\n",
    "        \"per:schools_attended\",\r\n",
    "        \"per:date_of_death\",\r\n",
    "        \"per:date_of_birth\",\r\n",
    "        \"per:place_of_birth\",\r\n",
    "        \"per:place_of_death\",\r\n",
    "        \"org:founded_by\",\r\n",
    "        \"per:religion\",\r\n",
    "    ]\r\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\r\n",
    "    label_indices = list(range(len(label_list)))\r\n",
    "    label_indices.remove(no_relation_label_idx)\r\n",
    "    # print(f'####### F1 {len(labels)} {len(preds)} {len(label_indices)}')\r\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\r\n",
    "\r\n",
    "\r\n",
    "def klue_re_auprc(probs, labels):\r\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\r\n",
    "    labels = np.eye(30)[labels]\r\n",
    "\r\n",
    "    score = np.zeros((30,))\r\n",
    "    for c in range(30):\r\n",
    "        targets_c = labels.take([c], axis=1).ravel()\r\n",
    "        preds_c = probs.take([c], axis=1).ravel()\r\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\r\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\r\n",
    "    return np.average(score) * 100.0\r\n",
    "\r\n",
    "\r\n",
    "def compute_metrics(pred):\r\n",
    "    \"\"\"validation을 위한 metrics function\"\"\"\r\n",
    "    # print(f'############## COMPUTE METRICS {len(pred.label_ids)} {len(pred.predictions.argmax(-1))} {len(pred.predictions)}')\r\n",
    "    labels = pred.label_ids\r\n",
    "    preds = pred.predictions.argmax(-1)\r\n",
    "    probs = pred.predictions\r\n",
    "\r\n",
    "    # calculate accuracy using sklearn's function\r\n",
    "    f1 = klue_re_micro_f1(preds, labels)\r\n",
    "    auprc = klue_re_auprc(probs, labels)\r\n",
    "    # acc = accuracy_score(labels, preds)  # 리더보드 평가에는 포함되지 않습니다.\r\n",
    "\r\n",
    "    return {\r\n",
    "        \"micro f1 score\": f1,\r\n",
    "        \"auprc\": auprc,\r\n",
    "        # \"accuracy\": acc,\r\n",
    "    }\r\n",
    "\r\n",
    "\r\n",
    "def label_to_num(label):\r\n",
    "    num_label = []\r\n",
    "    with open(\"dict_label_to_num.pkl\", \"rb\") as f:\r\n",
    "        dict_label_to_num = pickle.load(f)\r\n",
    "    for v in label:\r\n",
    "        num_label.append(dict_label_to_num[v])\r\n",
    "\r\n",
    "    return num_label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.autograd import Variable\r\n",
    "\r\n",
    "# https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\r\n",
    "class FocalLoss(nn.Module):\r\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\r\n",
    "        super(FocalLoss, self).__init__()\r\n",
    "        self.gamma = gamma\r\n",
    "        self.alpha = alpha\r\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\r\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\r\n",
    "        self.size_average = size_average\r\n",
    "\r\n",
    "    def forward(self, input, target):\r\n",
    "        if input.dim()>2:\r\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\r\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\r\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\r\n",
    "        target = target.view(-1,1)\r\n",
    "\r\n",
    "        logpt = F.log_softmax(input)\r\n",
    "        logpt = logpt.gather(1,target)\r\n",
    "        logpt = logpt.view(-1)\r\n",
    "        pt = Variable(logpt.data.exp())\r\n",
    "\r\n",
    "        if self.alpha is not None:\r\n",
    "            if self.alpha.type()!=input.data.type():\r\n",
    "                self.alpha = self.alpha.type_as(input.data)\r\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\r\n",
    "            logpt = logpt * Variable(at)\r\n",
    "\r\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\r\n",
    "        if self.size_average: return loss.mean()\r\n",
    "        else: return loss.sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# custom model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
<<<<<<< HEAD
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\"\"\" Define Custom Model -> will later allocated to models.py \"\"\"\n",
    "a = ''\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_name = CFG.model_name\n",
    "        self.num_labels = CFG.num_labels\n",
    "        self.input_size = CFG.input_size\n",
    "        self.output_size = CFG.output_size\n",
    "        self.num_rnn_layers = 3\n",
    "        self.dropout_rate = 0.1\n",
    "        self.is_train = True\n",
    "        self.backbone_model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "        # special_tokens_dict = {\n",
    "        #     'additional_special_tokens': CFG.special_token_list\n",
    "        # }\n",
    "        # num_added_tokens = self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        # self.backbone_model.resize_token_embeddings(len(self.tokenizer))\n",
    "        # add bidrectional gru (multiple) layers in the end\n",
    "\n",
    "        # self.lstm = nn.LSTM(\n",
    "        #     # set as BERT model's hidden size, not as an integer: flexible for different models\n",
    "        #     input_size=self.input_size,\n",
    "        #     hidden_size=self.output_size,\n",
    "        #     bidirectional=True,\n",
    "        #     batch_first=True,\n",
    "        #     num_layers=self.num_rnn_layers,\n",
    "        #     dropout=self.dropout_rate\n",
    "        #     )\n",
    "\n",
    "        # classifier은 바꾸지 않고\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            #torch.nn.Linear(self.output_size,self.output_size),\n",
    "            #torch.nn.ReLU(inplace=True),\n",
    "            #torch.nn.Dropout(p=self.dropout_rate, inplace=False),\n",
    "            torch.nn.Linear(self.output_size,self.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        backbone_output = self.backbone_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        global a\n",
    "        a = backbone_output\n",
    "        # add lstm layer\n",
    "        # _, (hn,cn) = self.lstm(backbone_output[0]) # torch.Size([batch, 132, 1024]) -> hn : torch.Size([6, batch, 768])\n",
    "        # hn[0] -> left to right, hn[1] -> right to left\n",
    "        \n",
    "        # b_n=hn.size()[1] # batch size\n",
    "        # hn=torch.transpose(hn, 0, 1).reshape(b_n, -1)   # torch.Size([batch, 6*768])\n",
    "        # hn=torch.transpose(hn, 1, 0).reshape(b_n, -1)   # torch.Size([batch, 6*768])\n",
    "\n",
    "        # input as fully connected layers\n",
    "        # output_lstm = torch.cat((hn[0], hn[1]), dim=1) # torch.Size([batch, 2*768])\n",
    "        output = self.classifier(backbone_output[0])    #  torch.Size([batch, 30])\n",
    "        # print(f'output : {output}')\n",
=======
    "from torch import nn\r\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\r\n",
    "\"\"\" Define Custom Model -> will later allocated to models.py \"\"\"\r\n",
    "\r\n",
    "class CustomModel(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.model_name = CFG.model_name\r\n",
    "        self.num_labels = CFG.num_labels\r\n",
    "        self.input_size = CFG.input_size\r\n",
    "        self.output_size = CFG.output_size\r\n",
    "        self.num_rnn_layers = 3\r\n",
    "        self.dropout_rate = 0.1\r\n",
    "        self.is_train = True\r\n",
    "        self.backbone_model = AutoModel.from_pretrained(self.model_name)\r\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\r\n",
    "        # special_tokens_dict = {\r\n",
    "        #     'additional_special_tokens': CFG.special_token_list\r\n",
    "        # }\r\n",
    "        # num_added_tokens = self.tokenizer.add_special_tokens(special_tokens_dict)\r\n",
    "        # self.backbone_model.resize_token_embeddings(len(self.tokenizer))\r\n",
    "        # add bidrectional gru (multiple) layers in the end\r\n",
    "\r\n",
    "        self.lstm = nn.LSTM(\r\n",
    "            # set as BERT model's hidden size, not as an integer: flexible for different models\r\n",
    "            input_size=1024,\r\n",
    "            hidden_size=self.output_size,\r\n",
    "            bidirectional=True,\r\n",
    "            batch_first=True,\r\n",
    "            num_layers=self.num_rnn_layers,\r\n",
    "            dropout=self.dropout_rate\r\n",
    "            )\r\n",
    "\r\n",
    "        # classifier은 바꾸지 않고\r\n",
    "        self.classifier = torch.nn.Sequential(\r\n",
    "            torch.nn.Linear(2*self.output_size,self.output_size),\r\n",
    "            torch.nn.ReLU(inplace=True),\r\n",
    "            torch.nn.Dropout(p=self.dropout_rate, inplace=False),\r\n",
    "            torch.nn.Linear(self.output_size,self.num_labels)\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, input_ids, attention_mask):\r\n",
    "        backbone_output = self.backbone_model(input_ids=input_ids, attention_mask=attention_mask)\r\n",
    "\r\n",
    "        # add lstm layer\r\n",
    "        _, (hn,cn) = self.lstm(backbone_output[0]) # torch.Size([batch, 132, 1024]) -> hn : torch.Size([6, batch, 768])\r\n",
    "        # hn[0] -> left to right, hn[1] -> right to left\r\n",
    "        \r\n",
    "        # b_n=hn.size()[1] # batch size\r\n",
    "        # hn=torch.transpose(hn, 0, 1).reshape(b_n, -1)   # torch.Size([batch, 6*768])\r\n",
    "        # hn=torch.transpose(hn, 1, 0).reshape(b_n, -1)   # torch.Size([batch, 6*768])\r\n",
    "\r\n",
    "        # input as fully connected layers\r\n",
    "        output_lstm = torch.cat((hn[0], hn[1]), dim=-1) # torch.Size([batch, 2*768])\r\n",
    "        output = self.classifier(output_lstm)    #  torch.Size([batch, 30])\r\n",
    "        # print(f'output : {output}')\r\n",
>>>>>>> cfe8d98ad3af3878dc1e0f40692011813d5e17b3
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = CustomModel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (backbone_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(1024, 1024, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = torch.randn(6, 32, 768)\r\n",
    "x=torch.transpose(x, 32, 0).reshape(32, -1)\r\n",
    "x.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 4608])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# from torchsummary import summary\r\n",
    "# summary(model,[(32,132)], [(32,132)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
<<<<<<< HEAD
    "training_args = TrainingArguments(\n",
    "    report_to = 'wandb',              \n",
    "    output_dir= CFG.result_dir,          # output directory\n",
    "    save_total_limit=5,              # number of total save model.\n",
    "    save_steps=CFG.save_steps,       # model saving step.\n",
    "    num_train_epochs=CFG.num_epochs,              # total number of training epochs\n",
    "    learning_rate=CFG.learning_rate,               # learning_rate\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    logging_dir= CFG.logging_dir, \n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    logging_steps=CFG.evaluation_steps,              # log saving step.\n",
    "    evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                                # `no`: No evaluation during training.\n",
    "                                # `steps`: Evaluate every `eval_steps`.\n",
    "                                # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps = CFG.evaluation_steps,            # evaluation step.\n",
    "    load_best_model_at_end = True,\n",
    "    group_by_length = True, # dynamic padding\n",
    "    warmup_steps=300,\n",
    "    dataloader_num_workers = CFG.num_workers,\n",
    "    metric_for_best_model = 'f1',\n",
    "    run_name = 'add_lstm',\n",
=======
    "training_args = TrainingArguments(\r\n",
    "    report_to = 'wandb',              \r\n",
    "    output_dir= CFG.result_dir,          # output directory\r\n",
    "    save_total_limit=5,              # number of total save model.\r\n",
    "    save_steps=CFG.save_steps,       # model saving step.\r\n",
    "    num_train_epochs=CFG.num_epochs,              # total number of training epochs\r\n",
    "    learning_rate=CFG.learning_rate,               # learning_rate\r\n",
    "    weight_decay=CFG.weight_decay,\r\n",
    "    logging_dir= CFG.logging_dir, \r\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\r\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\r\n",
    "    logging_steps=CFG.evaluation_steps,              # log saving step.\r\n",
    "    evaluation_strategy='steps', # evaluation strategy to adopt during training\r\n",
    "                                # `no`: No evaluation during training.\r\n",
    "                                # `steps`: Evaluate every `eval_steps`.\r\n",
    "                                # `epoch`: Evaluate every end of epoch.\r\n",
    "    eval_steps = CFG.evaluation_steps,            # evaluation step.\r\n",
    "    load_best_model_at_end = True,\r\n",
    "    group_by_length = True, # dynamic padding\r\n",
    "    warmup_steps=300,\r\n",
    "    dataloader_num_workers = CFG.num_workers,\r\n",
    "    metric_for_best_model = 'f1',\r\n",
    "    run_name = 'add_lstm',\r\n",
>>>>>>> cfe8d98ad3af3878dc1e0f40692011813d5e17b3
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "def makedirs(path) :\r\n",
    "    try :\r\n",
    "        os.makedirs(path)\r\n",
    "    except OSError :\r\n",
    "        if not os.path.isdir(path) :\r\n",
    "            raise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() and CFG.DEBUG == False else 'cpu')\r\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# model = CustomModel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "from transformers import Trainer\r\n",
    "\r\n",
    "loss_fn = FocalLoss(gamma=0.5)\r\n",
    "# loss_fn = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "class MyTrainer(Trainer):\r\n",
    "    def compute_loss(self, model, inputs, return_outputs=False) :\r\n",
    "        labels = inputs.pop('labels')\r\n",
    "        outputs = model(**inputs)\r\n",
    "        loss = loss_fn(outputs, labels)\r\n",
    "        return (loss, outputs) if return_outputs else loss\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
<<<<<<< HEAD
    "# dataset = pd.read_csv('/opt/ml/dataset/train/train_typed_entity_marker_punct.csv')\n",
    "dataset = pd.read_csv('/opt/ml/dataset/train/train_typed_entity_marker_punct.csv')\n",
    "dataset = dataset.sample(5000)\n",
    "dataset.reset_index(drop = True, inplace = True)"
=======
    "# dataset = pd.read_csv('/opt/ml/dataset/train/train_typed_entity_marker_punct.csv')\r\n",
    "dataset = pd.read_csv(PORORO_TRAIN_PATH)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset.info()\r\n",
    "len(dataset)"
>>>>>>> cfe8d98ad3af3878dc1e0f40692011813d5e17b3
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "models = []\r\n",
    "dataset = dataset[:5001]\r\n",
    "stf = StratifiedKFold(n_splits = 5)\r\n",
    "for fold, (train_idx, dev_idx) in enumerate(stf.split(dataset, list(dataset['label']))) :\r\n",
    "    print('Fold {}'.format(fold + 1))\r\n",
    "\r\n",
    "    model = CustomModel()\r\n",
    "    model.to(device)\r\n",
    "\r\n",
    "    # 추가한 token 개수만큼 token embedding size 늘려주기\r\n",
    "    # model.resize_token_embeddings(model.tokenizer.vocab_size + added_token_num)\r\n",
    "\r\n",
    "    train_dataset = dataset.iloc[train_idx]\r\n",
    "    dev_dataset = dataset.iloc[dev_idx]\r\n",
    "\r\n",
    "    train_label = label_to_num(train_dataset['label'].values)\r\n",
    "    dev_label = label_to_num(dev_dataset['label'].values)\r\n",
    "\r\n",
    "    tokenized_train = tokenized_dataset(train_dataset, model.tokenizer)\r\n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, model.tokenizer)\r\n",
    "\r\n",
    "    RE_train_dataset = RE_Dataset(tokenized_train, train_label)\r\n",
    "    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\r\n",
    "\r\n",
    "    trainer = MyTrainer(\r\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\r\n",
    "        args=training_args,                  # training arguments, defined above\r\n",
    "        train_dataset=RE_train_dataset,         # training dataset\r\n",
    "        eval_dataset=RE_dev_dataset,             # evaluation dataset\r\n",
    "        compute_metrics=compute_metrics,         # define metrics function\r\n",
    "    )\r\n",
    "   \r\n",
    "    trainer.train()\r\n",
    "    \r\n",
    "    # save model\r\n",
    "    makedirs(f'./best_model/lstm/fold_{fold}/')\r\n",
    "    model.save_pretrained(f'./best_model/lstm/fold_{fold}/')\r\n",
    "    \r\n",
    "    # Prevent OOM error\r\n",
    "    model.cpu()\r\n",
    "    del model\r\n",
    "    torch.cuda.empty_cache()\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleAttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'tokenizer'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-ebd0eca07345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdev_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtokenized_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    772\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "models = []\n",
    "stf = StratifiedKFold(n_splits = 5)\n",
    "for fold, (train_idx, dev_idx) in enumerate(stf.split(dataset, list(dataset['label']))) :\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "\n",
    "    model = CustomModel()\n",
    "    model.to(device)\n",
    "\n",
    "    # 추가한 token 개수만큼 token embedding size 늘려주기\n",
    "    # model.resize_token_embeddings(model.tokenizer.vocab_size + added_token_num)\n",
    "\n",
    "    train_dataset = dataset.iloc[train_idx]\n",
    "    dev_dataset = dataset.iloc[dev_idx]\n",
    "\n",
    "    train_label = label_to_num(train_dataset['label'].values)\n",
    "    dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "    tokenized_train = tokenized_dataset(train_dataset, model.tokenizer)\n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, model.tokenizer)\n",
    "\n",
    "    RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=RE_train_dataset,         # training dataset\n",
    "        eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "        compute_metrics=compute_metrics,         # define metrics function\n",
    "    )\n",
    "   \n",
    "    trainer.train()\n",
    "    \n",
    "    # save model\n",
    "    makedirs(f'./best_model/lstm/fold_{fold}/')\n",
    "    model.save_pretrained(f'./best_model/lstm/fold_{fold}/')\n",
    "    \n",
    "    # Prevent OOM error\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /opt/ml/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /opt/ml/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /opt/ml/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.1e08907f1658efd26357385b59d5a6567fc8faf443082618493ad3a2a1a45f0f\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 25976\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 77928\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "<ipython-input-9-92b0c0f817b9>:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='77928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  501/77928 01:39 < 4:17:31, 5.01 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6494' max='6494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6494/6494 04:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6494\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6494, 0]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-440c80dd53b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1446\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2052\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \u001b[0;31m# Metrics!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2292\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a3fe98b4d618>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# calculate accuracy using sklearn's function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklue_re_micro_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mauprc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklue_re_auprc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# acc = accuracy_score(labels, preds)  # 리더보드 평가에는 포함되지 않습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a3fe98b4d618>\u001b[0m in \u001b[0;36mklue_re_micro_f1\u001b[0;34m(preds, labels)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlabel_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_relation_label_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# print(f'####### F1 {len(labels)} {len(preds)} {len(label_indices)}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0mmodified\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \"\"\"\n\u001b[0;32m-> 1071\u001b[0;31m     return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n\u001b[0m\u001b[1;32m   1072\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \"\"\"\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001b[0m\u001b[1;32m   1196\u001b[0m                                                  \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0m\u001b[1;32m   1465\u001b[0m                                     pos_label)\n\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1275\u001b[0m                          str(average_options))\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6494, 0]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit"
  },
  "interpreter": {
   "hash": "b2ae1d7b084294015c4c9dcac235b5fd4686078c7a59d1ecf7082ca57a2117a9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}