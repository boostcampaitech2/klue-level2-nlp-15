[klue-level2-nlp-15](https://github.com/boostcampaitech2/klue-level2-nlp-15)

**dev branchì—ì„œ ì¶”ê°€ & ê°œì„ í•  ì **

- [x] entity_1, entity_2, [CLS]ì—ì„œ hidden state vectorì„ ë½‘ì•„ë‚´ì„œ concatí•˜ê¸°  -> ì„¸í˜„

  - [ ] Improved Baseline -> í˜„ìˆ˜

- [ ] **Bidirectional RNNì„ backbone ë’¤ì—ë‹¤ê°€ ë¶™ì—¬ë³´ê¸°** 

  ```python
  class RoBERTa(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.MODEL_NAME = 'klue/roberta-large'
          self.bert_model = AutoModel.from_pretrained(self.MODEL_NAME)
          self.hidden_size = 1024
          self.num_labels = 30
          self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL_NAME)
          
          special_tokens_dict = {
              'additional_special_tokens':[
                  '[SUB:ORG]',
                  '[SUB:PER]',
                  '[/SUB]',
                  '[OBJ:DAT]',
                  '[OBJ:LOC]',
                  '[OBJ:NOH]',
                  '[OBJ:ORG]',
                  '[OBJ:PER]',
                  '[OBJ:POH]',
                  '[/OBJ]'
              ]
          }
  
          num_added_tokens = self.tokenizer.add_special_tokens(special_tokens_dict)
          print("num_added_tokens:",num_added_tokens)
  
          self.bert_model.resize_token_embeddings(len(self.tokenizer))
          
          self.lstm = torch.nn.LSTM(input_size=~~~, hidden_size = ~~~,bidirectional=True)
          
          # classifierì€ ë°”ê¾¸ì§€ ì•Šê³ 
          self.classifier = torch.nn.Sequential(
              torch.nn.Linear(3*self.hidden_size,self.hidden_size),
              torch.nn.Dropout(p=0.1, inplace=False),
              torch.nn.Linear(self.hidden_size,self.num_labels)
          )
    def forward(self,item):
      input_ids = item['input_ids']
      token_type_ids = item['token_type_ids']
      attention_mask = item['attention_mask']
      sub_token_index = item['sub_token_index']
      obj_token_index = item['obj_token_index']
      out = self.bert_model(input_ids=input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask)
      h = out.last_hidden_state
      # output, _ = lstm(h,bidirectional=True)
      batch_size = h.shape[0]
  
  ```

  RBERTì—ì„œ ë‚˜ì˜¨ tokenì´ ë¬¸ì¥ ê¸¸ì´ 10ì´ì—ˆìœ¼ë©´ 10ê°œê°€ ìˆëŠ”ë°, ê·¸ê±¸ í•˜ë‚˜ì”© ë„£ìœ¼ë©´ì„œ 10ê°œ tokenì— ëŒ€í•´ì„œ sequenceì²˜ë¦¬ë¥¼ í•´ì•¼ í•  ê²ƒ ê°™ì•„ìš”.

- [ ] Dropout(p=0)ìœ¼ë¡œ Dropout ì œê±°

- [ ] AdamP ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤.

- [ ] í˜„ì¬ max_token_len = 256 -> collate_fn ë§Œë“¤ì–´ì„œ batchë§ˆë‹¤ max length ì •í•´ì„œ 125ë³´ë‹¤ ë” ì‘ì€ lengthì¸ ê²½ìš°ì—ëŠ” ë” ì§§ê²Œ í•´ì„œ í•™ìŠµì‹œí‚¤ë ¤ê³  í–ˆìŠµë‹ˆë‹¤ -> max_lengthë¥¼ êµ¬í•´ì„œ í•˜ë©´ ë²„ë¦¬ëŠ” ë°ì´í„°ë„ ì—†ê³  ì¢‹ì„ê²ƒê°™ì•„ìš” ì•„ì˜ˆ ë°ì´í„°ì…‹ì—ì„œ batchë¥¼ ë¬¶ì–´ì¤„ ë•Œ, ë¹„ìŠ·í•œ token ê°œìˆ˜(length)ì¸ ê²ƒë“¤ì„ ëª¨ì•„ì„œ ë½‘ëŠ” ë°©ì‹ë„ ìˆë”ë¼ê³ ìš”! ì´ê±¸ uniform length batchingì´ë¼ê³  í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì–´ì œ ì„±ìš±ë‹˜ì´ token length distributionì´ class ë³„ë¡œë„ ìœ ì‚¬í•˜ë‹¤ëŠ” ê±¸ ë³´ì—¬ì£¼ì‹  ë•ë¶„ì— uniform length batchingë„ ê±±ì •ì—†ì´ ì“¸ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ -> ì˜ì§„

- [ ] Train set ì „ì²´ë¥¼ ì…ë ¥í•˜ê³  Random Mask (prob=0.1) ì”Œì›Œì„œ Pretrainì„ í•˜ê³  ì‹¶ë‹¤. ë‚˜ë§Œì˜ ì‘ì€ KLUE-BERTë§Œë“¤ê¸° -> ì¤€í™, ì¬ì˜, ì˜ì§„ ğŸ¤—

  - [ ] ì‹¤í—˜í•  ë•ŒëŠ” ê°€ë²¼ìš´ ê±¸(klue/bert-base)ë¡œ ì‹¤í—˜í•˜ê³ , ë¬´ê±°ìš´ ê±°(xlm-roberta-large, klue/roberta-large)ë¡œ pretrainì‹œí‚¤ê¸°
  - [ ] ~~Train setí•˜ê³  validation setí•˜ê³  ë‚˜ëˆ ì„œ Validationí•´ì•¼ ì˜ë¯¸ê°€ ìˆëŠ” ë“¯... (ì§€ê¸ˆì€ ì¼ë‹¨ validationìœ¼ë¡œ ë‚˜ëˆ ë†“ëŠ”ë‹¤) ê·¼ë° ì •ë‹µ ì£¼ê³  í•™ìŠµí•˜ëŠ” ê±´ ì•„ë‹ˆë‹ˆê¹Œ ì¢€ ì• ë§¤... trainí•¨ìˆ˜ ì•ˆì— MLM trainí•  ìƒê°ì´ì—ˆëŠ”ë°. Train vs Valì„ ë‚˜ëˆ ë†“ì€ ë‹¤ìŒì— ê·¸ê±¸ train setì— MLMì— ì‚¬ìš©í•˜ê³ , MLMì´ ëë‚˜ë©´, finetuningì„ í•  ìƒê°ì´ì—ˆìŒ. random seedë§Œ í†µì¼ì‹œì¼œë†“ëŠ” ê²ƒ~~

- [ ] Pororoì—ì„œ NERë¡œ í‘œê¸°í•œ 42ê°œ ì¶”ê°€í•˜ê¸° 

  ```python
  # models.py
  class RoBERTa(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.MODEL_NAME = 'klue/roberta-large'
          self.bert_model = AutoModel.from_pretrained(self.MODEL_NAME)
          self.hidden_size = 1024
          self.num_labels = 30
          self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL_NAME)
          
          special_tokens_dict = {
              'additional_special_tokens':[
                  '[SUB:ORG]',
                  '[SUB:PER]',
                  '[/SUB]',
                  '[OBJ:DAT]',
                  '[OBJ:LOC]',
                  '[OBJ:NOH]',
                  '[OBJ:ORG]',
                  '[OBJ:PER]',
                  '[OBJ:POH]',
                  '[/OBJ]'
              ]
          }
  
  # dataset.pyì—ì„œ sub_type ë¶€ë¶„ì„ ë°”ê¾¸ë©´ ë¨
  def add_entity_token(data):
    """indexë¡œ í•˜ëŠ” ì´ìœ ê°€ ìˆë‹¤ê³ !"""
      sub_start_idx, sub_end_idx = data.subject_entity['start_idx'], data.subject_entity['end_idx']
      obj_start_idx, obj_end_idx = data.object_entity['start_idx'], data.object_entity['end_idx']
      
      sub_type = data.subject_entity['type']
      obj_type = data.object_entity['type']
      
      s = data.sentence
      
      if sub_start_idx < obj_start_idx:
          res = [
              s[:sub_start_idx],
              f"[SUB:{sub_type}]" + s[sub_start_idx:sub_end_idx+1] + "[/SUB]",
              s[sub_end_idx+1:obj_start_idx],
              f"[OBJ:{obj_type}]" + s[obj_start_idx:obj_end_idx+1] + "[/OBJ]",
              s[obj_end_idx+1:]
          ]
      else:
          res = [
              s[:obj_start_idx],
              f"[OBJ:{obj_type}]" + s[obj_start_idx:obj_end_idx+1] + "[/OBJ]",
              s[obj_end_idx+1:sub_start_idx],
              f"[SUB:{sub_type}]" + s[sub_start_idx:sub_end_idx+1] + "[/SUB]",
              s[sub_end_idx+1:]
          ]
      
      return ''.join(res)    
  
  ```

- [ ] [Stratified K-Fold ì¶”ê°€: ì°¸ê³  ì‚¬í•­](https://github.com/boostcampaitech2/klue-level2-nlp-15/blob/main/train_with_pororo.ipynb)

---

- [x] ~~Multitask Classificationì„ êµ¬í˜„í•˜ê³  ì‹¶ë‹¤. (Duo classifier)~~
- [x] ~~KoElectra as backbone model~~

- [x] Data Augmentation
  - [x] KoEDA - Random Switching, ë™ì˜ì–´ ë°”ê¾¸ê¸° -> CSVë¡œ ê³µìœ í•´ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŒë‹¤ ã…ã…ã…
  - [x] pororoë¥¼ ì´ìš©í•´ì„œ round trip translationì„ í•˜ê³  ì‹¶ë‹¤ -> ì˜ì§„
  
  
