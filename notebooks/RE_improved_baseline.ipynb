{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39aa893c-6a22-411b-b6a7-9c512b3b10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ast import literal_eval\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.cuda.amp import GradScaler\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef33138-72d3-49ea-90b9-0ce7e2cf538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOCAL LOSS ###\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737e377-24df-4c03-aaa7-b5a3f054bb3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5709895-e377-4fea-b4d5-b7b51588e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "    subject_entity = []\n",
    "    object_entity = []\n",
    "    for i, j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "        sub_dict = literal_eval(i)\n",
    "        obj_dict = literal_eval(j)\n",
    "\n",
    "        sub_start = int(sub_dict['start_idx'])\n",
    "        sub_end = int(sub_dict['end_idx'])\n",
    "        sub_type = sub_dict['type']\n",
    "\n",
    "        obj_start = int(obj_dict['start_idx'])\n",
    "        obj_end = int(obj_dict['end_idx'])\n",
    "        obj_type = obj_dict['type']\n",
    "\n",
    "        subject_entity.append([sub_start, sub_end, sub_type])\n",
    "        object_entity.append([obj_start, obj_end, obj_type])\n",
    "    out_dataset = pd.DataFrame({'id': dataset['id'],\n",
    "                                'sentence': dataset['sentence'],\n",
    "                                'subject_entity': subject_entity,\n",
    "                                'object_entity': object_entity,\n",
    "                                'label': dataset['label'], })\n",
    "    return out_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6baa4e1-bac4-4d16-9eff-c6794ab8b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    dataset = preprocessing_dataset(pd_dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    각 문장에 typed entity marker를 끼워줍니다.\n",
    "    entity type는 한국어모델에 맞게 한 토큰으로\n",
    "    토크나이징되는 단얼로 대체 (e.g. \"PER\" -> \"사람\")\n",
    "    subject: @*type*subject word@ (e.g.  김현수 -> @*사람*김현수@)\n",
    "    object: #^type^object word# (e.g. #^지명^한국#)\n",
    "\n",
    "    <<An Improved Baseline for Sentence-level Relation Extraction>>\n",
    "    논문에서 각 entity marker의 시작위치 (ss, es)를 사용하기 때문에 함께 반환\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    type_dict = {\n",
    "        \"PER\": \"사람\",\n",
    "        \"LOC\": \"지명\",\n",
    "        \"ORG\": \"기관\",\n",
    "        \"DAT\": \"날짜\",\n",
    "        \"TIM\": \"시간\",\n",
    "        \"DUR\": \"기간\",\n",
    "        \"MNY\": \"통화\",\n",
    "        \"PNT\": \"비율\",\n",
    "        \"NOH\": \"수량\",\n",
    "        \"POH\": \"기타\"\n",
    "    }\n",
    "    sentences = []\n",
    "    e01, e02, sent = dataset['subject_entity'], dataset['object_entity'], dataset['sentence']\n",
    "    subject_start, subject_end, sub_type = e01\n",
    "    object_start, object_end, obj_type = e02\n",
    "    subj = sent[e01[0]: e01[1] + 1]\n",
    "    obj = sent[e02[0]: e02[1] + 1]\n",
    "    if subject_start < object_start:\n",
    "        sent_ = sent[:subject_start] + f'@*{type_dict[sub_type]}*' + subj + '@' + \\\n",
    "                    sent[subject_end + 1:object_start] + f'&^{type_dict[obj_type]}^' \\\n",
    "                    + obj + '&' + sent[object_end + 1:]\n",
    "        ss = 1 + len(tokenizer.tokenize(sent[:subject_start]))\n",
    "        se = ss + 4 + len(tokenizer.tokenize(subj))\n",
    "        es = 1 + se + len(tokenizer.tokenize(sent[subject_end + 1:object_start]))\n",
    "        ee = es + 4 + len(tokenizer.tokenize(obj))\n",
    "    else:\n",
    "        sent_ = sent[:object_start] + f'&^{type_dict[obj_type]}^' + obj + '&' + \\\n",
    "                sent[object_end + 1:subject_start] + f'@*{type_dict[sub_type]}*' + subj + '@' + \\\n",
    "                sent[subject_end + 1:]\n",
    "        es = 1 + len(tokenizer.tokenize(sent[:object_start]))\n",
    "        ee = es + 4 + len(tokenizer.tokenize(obj))\n",
    "        ss = 1 + ee + len(tokenizer.tokenize(sent[object_end + 1:subject_start]))\n",
    "        se = ss + 4 + len(tokenizer.tokenize(subj))\n",
    "    sentences.append(sent_)\n",
    "    max_length = 256\n",
    "    senttokens = tokenizer.tokenize(sent_)[:max_length - 2]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(senttokens)\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)\n",
    "    return input_ids, ss, se, es, ee\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ''' 각 batch안에서\n",
    "    max_len = max([len(f['input_ids'] for f in batch])식으로\n",
    "    맞추려고 했는데 잘 안되서 일단 max_len 상수값으로 지정\n",
    "    '''\n",
    "    max_len = 256\n",
    "    input_ids = [f[\"input_ids\"] + [1] * (max_len - len(f[\"input_ids\"])) for f in batch]\n",
    "    #input_ids = [f[\"input_ids\"] + [0] * (max_len - len(f[\"input_ids\"])) for f in batch]\n",
    "    input_mask = [[1.0] * len(f[\"input_ids\"]) + [0.0] * (max_len - len(f[\"input_ids\"])) for f in batch]\n",
    "    labels = [f[\"labels\"] for f in batch]\n",
    "    ss = [f[\"ss\"] for f in batch]\n",
    "    se = [f['se'] for f in batch]\n",
    "    es = [f[\"es\"] for f in batch]\n",
    "    ee = [f['ee'] for f in batch]\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    input_mask = torch.tensor(input_mask, dtype=torch.float)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    ss = torch.tensor(ss, dtype=torch.long)\n",
    "    se = torch.tensor(se, dtype=torch.long)\n",
    "    es = torch.tensor(es, dtype=torch.long)\n",
    "    ee = torch.tensor(ee, dtype=torch.long)\n",
    "    output = (input_ids, input_mask, labels, ss, se, es, ee)\n",
    "    return output\n",
    "\n",
    "\n",
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('./code/dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "    return num_label\n",
    "\n",
    "\n",
    "def processor(tokenizer, dataset, train_mode):\n",
    "    '''\n",
    "    train_dataset = processor(tokenizer, train_df))\n",
    "    --> train_dataloader = Dataloader(train_dataset, batch_size = ...)\n",
    "    '''\n",
    "    features = []\n",
    "    labels = dataset['label'].values\n",
    "    if train_mode:\n",
    "        labels = label_to_num(dataset['label'].values)\n",
    "    for i in range(len(dataset)):\n",
    "        input_ids, new_ss, new_se, new_es, new_ee = tokenized_dataset(dataset.iloc[i], tokenizer)\n",
    "        label = labels[i]\n",
    "        feature = {\n",
    "            'input_ids' : input_ids,\n",
    "            'labels' : label,\n",
    "            'ss': new_ss,\n",
    "            'se': new_se,\n",
    "            'es' : new_es,\n",
    "            'ee' : new_ee,\n",
    "        }\n",
    "        features.append(feature)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21cf940-8574-4bbd-8a24-45e430fc7adb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c30e085-900f-436e-bd2e-5673e96b6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name, config):\n",
    "        super().__init__()\n",
    "        self.encoder_model = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.loss_fnt = FocalLoss(gamma = 1.0)\n",
    "        #self.loss_fnt = nn.CrossEntropyLoss(class_weights)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size*2, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(hidden_size, config.num_labels)\n",
    "        )\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, input_ids=None,\n",
    "                attention_mask=None,\n",
    "                labels=None,\n",
    "                ss=None,\n",
    "                se = None,\n",
    "                es=None,\n",
    "                ee = None,):\n",
    "        \n",
    "        outputs = self.encoder_model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        last_hs = outputs[0]\n",
    "        idx = torch.arange(input_ids.size(0)).to(input_ids.device)\n",
    "        ss_emb = last_hs[idx, ss]\n",
    "        #se_emb = pooled_output[idx,se]\n",
    "        es_emb = last_hs[idx, es]\n",
    "        #ee_emb = pooled_output[idx, ee]\n",
    "        h = torch.cat((ss_emb,es_emb), dim=-1)\n",
    "        #final = torch.cat((cls, h), dim=-1)\n",
    "        logits = self.classifier(h)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fnt(logits.float(), labels)\n",
    "            outputs = (loss,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46ad57-44df-453b-9a29-16fdd6d36407",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2c0286-9f4c-4729-831a-5dadd68492bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministric = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "                  'org:product', 'per:title', 'org:alternate_names',\n",
    "                  'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "                  'org:number_of_employees/members', 'per:children',\n",
    "                  'per:place_of_residence', 'per:alternate_names',\n",
    "                  'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "                  'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "                  'org:member_of', 'per:parents', 'org:dissolved',\n",
    "                  'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "                  'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "                  'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    probs = np.array(probs)\n",
    "    labels = np.eye(30)[labels]\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "\n",
    "def compute_metrics(keys, logitss):\n",
    "    \"\"\" validation을 위한 metrics function \"\"\"\n",
    "    #print(pred.predictions[0])\n",
    "    labels = np.array(keys, dtype= np.int64)\n",
    "    logitss = torch.tensor(logitss)\n",
    "    preds = torch.argmax(logitss, dim= -1)\n",
    "    probs = logitss\n",
    "\n",
    "    # calculate accuracy using sklearn's function\n",
    "    f1 = klue_re_micro_f1(preds, labels)\n",
    "    auprc = klue_re_auprc(probs, labels)\n",
    "    acc = accuracy_score(labels, preds)  # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "    return f1, auprc, acc\n",
    "    #return {\n",
    "    #    'micro f1 score': f1,\n",
    "    #    'auprc': auprc,\n",
    "    #    'accuracy': acc,\n",
    "    #}\n",
    "\n",
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('./code/dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "    return num_label\n",
    "\n",
    "\n",
    "def split_df(df, kfold_n):\n",
    "    kfold = StratifiedKFold(n_splits = kfold_n)\n",
    "    X = df['sentence'].values\n",
    "    y = df['label'].values\n",
    "    datas = []\n",
    "    for i, (train_index, valid_index) in enumerate(kfold.split(X,y)):\n",
    "        train_df = df.iloc[train_index].copy().reset_index(drop=True)\n",
    "        valid_df = df.iloc[valid_index].copy().reset_index(drop=True)\n",
    "\n",
    "        datas.append((train_df, valid_df))\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b483d-6c1d-4b3f-ac3a-f51cbbf9fb4c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae22663-c085-4bb6-9223-a8190836683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features):\n",
    "    dataloader = DataLoader(features, batch_size = 5, collate_fn = collate_fn, drop_last = False)\n",
    "    keys, preds, logitss = [], [], []\n",
    "    device = torch.device(\"cuda\")\n",
    "    for i_b, batch in enumerate(dataloader):\n",
    "        model.eval()\n",
    "        inputs = {'input_ids': batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'ss': batch[3].to(device),\n",
    "                  'es': batch[5].to(device),\n",
    "                  }\n",
    "        keys+= batch[2].tolist()\n",
    "        with torch.no_grad():\n",
    "            logit = model(**inputs)[0]\n",
    "            pred = torch.argmax(logit, dim=-1)\n",
    "            for i in logit:\n",
    "                logitss.append(i.tolist())\n",
    "        preds += pred.tolist()\n",
    "\n",
    "    keys = np.array(keys, dtype = np.int64)\n",
    "    f1, auprc, acc = compute_metrics(keys, logitss)\n",
    "    output = {\n",
    "        \"f1\": f1, \"auprc\": auprc, \"acc\" : acc\n",
    "    }\n",
    "    print(output)\n",
    "    return f1, auprc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53831a71-7dda-4df8-87cd-1d014ecd9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # load model and tokenizer\n",
    "    MODEL_NAME = \"klue/roberta-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    dataset = load_data(\"./dataset/train/train.csv\")\n",
    "\n",
    "    # Make Validation set\n",
    "    kfold_dataset = split_df(dataset, kfold_n = 5)\n",
    "    train_dataset  = kfold_dataset[-1][0]\n",
    "    valid_dataset = kfold_dataset[-1][1]\n",
    "\n",
    "    # tokenizing dataset & making train dataloader\n",
    "    train_features = processor(tokenizer, dataset, train_mode = True)\n",
    "    val_features = processor(tokenizer, valid_dataset, train_mode = True)\n",
    "    train_dataloader = DataLoader(train_features, batch_size= 16, shuffle=True, collate_fn = collate_fn)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 30\n",
    "    model = CustomModel(MODEL_NAME, config = model_config)\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    ### Hyper-parameters to add in argmument parser ##\n",
    "    num_train_epochs = 10\n",
    "    gradient_accumulation_steps = 2\n",
    "    adam_epsilon = 1e-6\n",
    "    max_grad_norm = 1.0\n",
    "    warmup_ratio = 0.1\n",
    "    learning_rate = 3e-5\n",
    "    ##################################\n",
    "\n",
    "    total_steps = int(len(train_dataloader) * num_train_epochs) // gradient_accumulation_steps\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scaler = GradScaler()\n",
    "    #optimizer = AdamW(model.parameters(), \n",
    "    #                  lr = learning_rate, \n",
    "    #                  eps = adam_epsilon)\n",
    "    optimizer = AdamP(model.parameters(), \n",
    "                      lr=learning_rate, betas=(0.9, 0.999), \n",
    "                      weight_decay=1e-2)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps = total_steps\n",
    "                                                )\n",
    "    num_steps = 0\n",
    "    best_f1 = 0\n",
    "    for epoch in range(int(num_train_epochs)):\n",
    "        model.zero_grad()\n",
    "        average_loss = 0\n",
    "        #tokens = []\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            model.train()\n",
    "            inputs = {'input_ids': batch[0].to(device),\n",
    "                      'attention_mask': batch[1].to(device),\n",
    "                      'labels': batch[2].to(device),\n",
    "                      'ss': batch[3].to(device),\n",
    "                      'es': batch[5].to(device),\n",
    "                      }\n",
    "            ''' 올바른 토큰 참조하는지 확인용\n",
    "            for i in range(len(batch[0])):\n",
    "                ss_ = tokenizer.decode(batch[0][i])\n",
    "                ss = tokenizer.tokenize(ss_)[batch[3][i]]\n",
    "                se = tokenizer.tokenize(ss_)[batch[4][i]]\n",
    "                e_ = tokenizer.decode(batch[0][i])\n",
    "                es = tokenizer.tokenize(e_)[batch[5][i]]\n",
    "                ee = tokenizer.tokenize(e_)[batch[6][i]]\n",
    "                tok = \"\".join([ss,se,es,ee])\n",
    "                tokens.append(tok)\n",
    "            '''\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0] / gradient_accumulation_steps\n",
    "            average_loss += loss\n",
    "            scaler.scale(loss).backward()\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                num_steps += 1\n",
    "                if max_grad_norm > 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(),max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "        print(f\"average_training_loss: {average_loss/len(train_dataloader)}\")\n",
    "        #toks_acc = Counter(tokens)\n",
    "        #print(toks_acc)\n",
    "        ### Validation ###\n",
    "        f1, auprc, acc = evaluate(model, val_features)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model, './best_model/roberta_focal.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "709a85a9-39a4-400f-93b9-bab4af167df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ec517-3bd6-4ea2-9904-3e28ca3f3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
