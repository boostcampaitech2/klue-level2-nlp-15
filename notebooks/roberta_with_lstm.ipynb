{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Define paths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\"\"\" define train data and test data path \"\"\"\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# root path\n",
    "ROOT_PATH = os.path.abspath(\".\")\n",
    "\n",
    "# directory paths\n",
    "ROOT_DATA_DIR = os.path.join(os.path.dirname(ROOT_PATH), 'dataset')\n",
    "TRAIN_DATA_DIR = os.path.join(ROOT_DATA_DIR, 'train')\n",
    "TEST_DATA_DIR = os.path.join(ROOT_DATA_DIR, 'test')\n",
    "PRIDICTION_DIR = os.path.join(os.path.dirname(ROOT_PATH), 'prediction')\n",
    "BASELINE_DIR = os.path.join(os.path.dirname(ROOT_PATH), 'code')\n",
    "\n",
    "# files paths\n",
    "TRAIN_FILE_PATH = glob(os.path.join(TRAIN_DATA_DIR, '*'))[0]\n",
    "TEST_FILE_PATH = glob(os.path.join(TEST_DATA_DIR, '*'))[0]\n",
    "SAMPLE_SUBMISSION_PATH = os.path.join(PRIDICTION_DIR, 'sample_submission.csv')\n",
    "PORORO_TRAIN_PATH = os.path.join(TRAIN_DATA_DIR, 'pororo_train_typed_entity_marker_punct.csv')\n",
    "PORORO_TEST_PATH = os.path.join(TEST_DATA_DIR, 'pororo_test_typed_entity_marker_punct.csv')\n",
    "\n",
    "print(TRAIN_FILE_PATH, TEST_FILE_PATH,SAMPLE_SUBMISSION_PATH, PORORO_TRAIN_PATH, PORORO_TEST_PATH)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/dataset/train/train.csv /opt/ml/dataset/test/test_data.csv /opt/ml/prediction/sample_submission.csv /opt/ml/dataset/train/pororo_train_typed_entity_marker_punct.csv /opt/ml/dataset/test/pororo_test_typed_entity_marker_punct.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# set config"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "\"\"\" Set configuration as dictionary format \"\"\"\n",
    "\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "# login wandb and get today's date until hour and minute\n",
    "wandb.login()\n",
    "today = datetime.now().strftime(\"%m%d_%H:%M\")\n",
    "\n",
    "# Debug set to true in order to debug high-layer code.\n",
    "# CFG Configuration\n",
    "CFG = wandb.config # wandb.config provides functionality of easydict.EasyDict\n",
    "CFG.DEBUG = False\n",
    "\n",
    "# Dataset Config as constants\n",
    "CFG.num_labels = 30\n",
    "CFG.num_workers = 4\n",
    "CFG.split_ratio = 0 # not going to use validation/test set\n",
    "CFG.batch_size = 32\n",
    "\n",
    "# Train configuration\n",
    "CFG.user_name = \"kyeonj\"\n",
    "CFG.file_base_name = f\"{CFG.user_name}_{today}\"\n",
    "# CFG.model_name = \"klue/roberta-base\" # https://huggingface.co/klue/roberta-base\n",
    "# CFG.model_name = \"monologg/koelectra-base-v3-discriminator\" # https://huggingface.co/monologg/koelectra-base-v3-discriminator\n",
    "# CFG.model_name = \"klue/roberta-large\"\n",
    "CFG.model_name = \"klue/roberta-large\"\n",
    "CFG.num_folds = 5 # 5 Fold as default\n",
    "CFG.num_epochs = 3 # \n",
    "CFG.max_token_length = 128 # refer to EDA where Q3 is 119, there are only 460 sentences out of 32k train set\n",
    "CFG.stopwords = []\n",
    "CFG.learning_rate = 5e-5\n",
    "CFG.weight_decay = 1e-2 # https://paperswithcode.com/method/weight-decay\n",
    "CFG.input_size = 768\n",
    "CFG.output_size = 768\n",
    "CFG.num_rnn_layers = 3\n",
    "CFG.dropout_rate = 0.0\n",
    "\n",
    "# training steps configurations\n",
    "CFG.save_steps = 500\n",
    "CFG.early_stopping_patience = 5\n",
    "CFG.warmup_steps = 500\n",
    "CFG.logging_steps = 100\n",
    "CFG.evaluation_strategy = 'epoch'\n",
    "CFG.evaluation_steps = 500\n",
    "\n",
    "# Directory configuration\n",
    "CFG.result_dir = os.path.join(os.path.dirname(ROOT_PATH), \"results\")\n",
    "CFG.saved_model_dir = os.path.join(os.path.dirname(ROOT_PATH), \"best_models\")\n",
    "CFG.logging_dir = os.path.join(os.path.dirname(ROOT_PATH), \"logs\")\n",
    "CFG.baseline_dir = os.path.join(os.path.dirname(ROOT_PATH), 'baseline-code')\n",
    "\n",
    "# file configuration\n",
    "CFG.result_file = os.path.join(CFG.result_dir, f\"{CFG.file_base_name}.csv\")\n",
    "CFG.saved_model_file = os.path.join(CFG.saved_model_dir, f\"{CFG.file_base_name}.pth\")\n",
    "CFG.logging_file = os.path.join(CFG.logging_dir, f\"{CFG.file_base_name}.log\")\n",
    "CFG.label_to_num_file = os.path.join(CFG.baseline_dir, 'dict_label_to_num.pkl')\n",
    "CFG.num_to_label_file = os.path.join(CFG.baseline_dir, \"dict_num_to_label.pkl\")\n",
    "CFG.train_file_path = TRAIN_FILE_PATH\n",
    "CFG.test_file_path = TEST_FILE_PATH\n",
    "CFG.sample_submission_file_path = SAMPLE_SUBMISSION_PATH\n",
    "\n",
    "# Other configurations\n",
    "CFG.load_best_model_at_end = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "eng_to_kor = {\n",
    "    'quantity' : 'Î¨ºÎüâ'\n",
    "    ,'person' : 'ÏÇ¨Îûå'\n",
    "    ,'term' : 'ÏûÑÍ∏∞'\n",
    "    ,'o' : 'ÏóÜÏùå'\n",
    "    ,'event' : 'Ïù¥Î≤§Ìä∏'\n",
    "    ,'study_field':'Ïä§ÌÑ∞ÎîîÌïÑÎìú'\n",
    "    ,'material':'Ïû¨Î£å'\n",
    "    ,'city':'ÎèÑÏãú'\n",
    "    ,'time':'ÏãúÍ∞Ñ'\n",
    "    ,'animal':'ÎèôÎ¨º'\n",
    "    ,'location':'ÏúÑÏπò'\n",
    "    ,'disease': 'ÏßàÎ≥ë'\n",
    "    ,'civilization':'Î¨∏Î™Ö'\n",
    "    ,'occupation':'ÏßÅÏóÖ'\n",
    "    ,'organization':'Ï°∞ÏßÅ'\n",
    "    ,'country':'ÎÇòÎùº'\n",
    "    ,'artifact':'Ïú†Î¨º'\n",
    "    ,'date':'ÎÇ†Ïßú'\n",
    "    ,'plant':'ÏãùÎ¨º'\n",
    "    ,'theory':'Ïù¥Î°†'\n",
    "}\n",
    "\n",
    "CFG.special_token_list = []\n",
    "for v in eng_to_kor.values():\n",
    "    CFG.special_token_list.append(\"*\"+v+\"*\")\n",
    "    CFG.special_token_list.append(\"^\"+v+\"^\")   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import StratifiedKFold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed) :\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def pull_out_dictionary(df_input: pd.DataFrame):\n",
    "  \"\"\" pull out str `{}` values from the pandas dataframe and shape it as a new column\"\"\"\n",
    "\n",
    "  df = df_input.copy()\n",
    "\n",
    "  # assign subject_entity and object_entity column values type as dictionary\n",
    "  df['subject_entity'] = df['subject_entity'].apply(lambda x: eval(x))\n",
    "  df['object_entity'] = df['object_entity'].apply(lambda x: eval(x))\n",
    "\n",
    "  # parse item inside of subject_entity and object_entity's dictionary values as columns of dataframe\n",
    "  # word, start_idx, end_idx, type as new columns \n",
    "  df = df.assign(\n",
    "      # subject_entity\n",
    "      subject_word=lambda x: x['subject_entity'].apply(lambda x: x['word']),\n",
    "      subject_start_idx=lambda x: x['subject_entity'].apply(lambda x: x['start_idx']),\n",
    "      subject_end_idx=lambda x: x['subject_entity'].apply(lambda x: x['end_idx']),\n",
    "      subject_type=lambda x: x['subject_entity'].apply(lambda x: x['type']),\n",
    "      \n",
    "      # object_entity\n",
    "      object_word=lambda x: x['object_entity'].apply(lambda x: x['word']),\n",
    "      object_start_idx=lambda x: x['object_entity'].apply(lambda x: x['start_idx']),\n",
    "      object_end_idx=lambda x: x['object_entity'].apply(lambda x: x['end_idx']),\n",
    "      object_type=lambda x: x['object_entity'].apply(lambda x: x['type']),\n",
    "  )\n",
    "\n",
    "  # drop subject_entity and object_entity column\n",
    "  df = df.drop(['subject_entity', 'object_entity'], axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset Íµ¨ÏÑ±ÏùÑ ÏúÑÌïú class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "def preprocessing_dataset(dataset):\n",
    "  \"\"\" Ï≤òÏùå Î∂àÎü¨Ïò® csv ÌååÏùºÏùÑ ÏõêÌïòÎäî ÌòïÌÉúÏùò DataFrameÏúºÎ°ú Î≥ÄÍ≤Ω ÏãúÏºúÏ§çÎãàÎã§.\"\"\"\n",
    "  \n",
    "  dataset = pull_out_dictionary(dataset)\n",
    "\n",
    "  # rename columns subject_word as subject_entity, object_word as object_entity\n",
    "  dataset = dataset.rename(columns={'subject_word': 'subject_entity', 'object_word': 'object_entity'})\n",
    "\n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':dataset['subject_entity'],'object_entity':dataset['object_entity'],'label':dataset['label'],})\n",
    "  display(out_dataset.head(2))\n",
    "  return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "  \"\"\" csv ÌååÏùºÏùÑ Í≤ΩÎ°úÏóê Îß°Í≤å Î∂àÎü¨ ÏòµÎãàÎã§. \"\"\"\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\n",
    "  dataset = preprocessing_dataset(pd_dataset)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  \"\"\" tokenizerÏóê Îî∞Îùº sentenceÎ•º tokenizing Ìï©ÎãàÎã§.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      max_length=CFG.max_token_length + 4,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      )\n",
    "  return tokenized_sentences\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "# from load_data import *\n",
    "\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "       'org:product', 'per:title', 'org:alternate_names',\n",
    "       'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "       'org:number_of_employees/members', 'per:children',\n",
    "       'per:place_of_residence', 'per:alternate_names',\n",
    "       'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "       'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "       'org:member_of', 'per:parents', 'org:dissolved',\n",
    "       'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "       'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "       'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = klue_re_micro_f1(preds, labels)\n",
    "    return {\"f1 score\": acc}\n",
    "\n",
    "def label_to_num(label=None):\n",
    "  num_label = []\n",
    "  with open(os.path.join(BASELINE_DIR, \"dict_label_to_num.pkl\"), 'rb') as f:\n",
    "    dict_label_to_num = pickle.load(f)\n",
    "  for v in label:\n",
    "    num_label.append(dict_label_to_num[v])\n",
    "  \n",
    "  return num_label\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# custom model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "\"\"\" Define Custom Model -> will later allocated to models.py \"\"\"\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.model_name = CFG.model_name\n",
    "        self.num_labels = CFG.num_labels\n",
    "        self.input_size = CFG.input_size\n",
    "        self.output_size = CFG.output_size \n",
    "        self.num_rnn_layers = 3,\n",
    "        self.dropout_rate = 0,\n",
    "        self.is_train = True\n",
    "\n",
    "        self.backbone_model = AutoModel.from_pretrained(self.model_name)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "        special_tokens_dict = {\n",
    "            'additional_special_tokens': CFG.special_token_list\n",
    "        }        \n",
    "        num_added_tokens = self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        self.backbone_model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        # add bidrectional gru (multiple) layers in the end\n",
    "        self.lstm = nn.LSTM(\n",
    "            # set as BERT model's hidden size, not as an integer: flexible for different models\n",
    "            input_size=1024, \n",
    "            hidden_size=self.output_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True, \n",
    "            num_layers=1\n",
    "            # dropout=dropout_rate\n",
    "            )\n",
    "        \n",
    "        # classifierÏùÄ Î∞îÍæ∏ÏßÄ ÏïäÍ≥†\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*self.output_size,self.output_size),\n",
    "            # torch.nn.Dropout(p=self.dropout_rate, inplace=False),\n",
    "            torch.nn.Linear(self.output_size,self.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        backbone_output = self.backbone_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # add lstm layer\n",
    "        # output, (hn, cn) = rnn(inp, (h0, c0))\n",
    "        # output2, _ = rnn_two(output)\n",
    "\n",
    "        lstm_output = self.lstm(backbone_output[0]) # (output, )\n",
    "        # flatten the output\n",
    "        lstm_output = lstm_output[0]\n",
    "        # input as fully connected layers\n",
    "        output = self.classifier(lstm_output)\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "model = CustomModel()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "training_args = TrainingArguments(\n",
    "  report_to = 'wandb',              \n",
    "  output_dir= CFG.result_dir,          # output directory\n",
    "  save_total_limit=5,              # number of total save model.\n",
    "  save_steps=CFG.save_steps,       # model saving step.\n",
    "  num_train_epochs=CFG.num_epochs,              # total number of training epochs\n",
    "  learning_rate=CFG.learning_rate,               # learning_rate\n",
    "  weight_decay=CFG.weight_decay,\n",
    "  logging_dir= CFG.logging_dir, \n",
    "  per_device_train_batch_size=CFG.batch_size,  # batch size per device during training\n",
    "  per_device_eval_batch_size=CFG.batch_size,   # batch size for evaluation\n",
    "  logging_steps=CFG.evaluation_steps,              # log saving step.\n",
    "  evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                              # `no`: No evaluation during training.\n",
    "                              # `steps`: Evaluate every `eval_steps`.\n",
    "                              # `epoch`: Evaluate every end of epoch.\n",
    "  eval_steps = CFG.evaluation_steps,            # evaluation step.\n",
    "  load_best_model_at_end = True,\n",
    "  group_by_length = True, # dynamic padding\n",
    "  warmup_steps=300,\n",
    "  dataloader_num_workers = CFG.num_workers,\n",
    "  metric_for_best_model = 'f1',\n",
    "  run_name = 'add_lstm',\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def makedirs(path) :\n",
    "    try :\n",
    "        os.makedirs(path)\n",
    "    except OSError :\n",
    "        if not os.path.isdir(path) :\n",
    "            raise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() and CFG.DEBUG == False else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "model = CustomModel()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /opt/ml/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /opt/ml/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /opt/ml/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.1e08907f1658efd26357385b59d5a6567fc8faf443082618493ad3a2a1a45f0f\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Assigning ['*Î¨ºÎüâ*', '^Î¨ºÎüâ^', '*ÏÇ¨Îûå*', '^ÏÇ¨Îûå^', '*ÏûÑÍ∏∞*', '^ÏûÑÍ∏∞^', '*ÏóÜÏùå*', '^ÏóÜÏùå^', '*Ïù¥Î≤§Ìä∏*', '^Ïù¥Î≤§Ìä∏^', '*Ïä§ÌÑ∞ÎîîÌïÑÎìú*', '^Ïä§ÌÑ∞ÎîîÌïÑÎìú^', '*Ïû¨Î£å*', '^Ïû¨Î£å^', '*ÎèÑÏãú*', '^ÎèÑÏãú^', '*ÏãúÍ∞Ñ*', '^ÏãúÍ∞Ñ^', '*ÎèôÎ¨º*', '^ÎèôÎ¨º^', '*ÏúÑÏπò*', '^ÏúÑÏπò^', '*ÏßàÎ≥ë*', '^ÏßàÎ≥ë^', '*Î¨∏Î™Ö*', '^Î¨∏Î™Ö^', '*ÏßÅÏóÖ*', '^ÏßÅÏóÖ^', '*Ï°∞ÏßÅ*', '^Ï°∞ÏßÅ^', '*ÎÇòÎùº*', '^ÎÇòÎùº^', '*Ïú†Î¨º*', '^Ïú†Î¨º^', '*ÎÇ†Ïßú*', '^ÎÇ†Ïßú^', '*ÏãùÎ¨º*', '^ÏãùÎ¨º^', '*Ïù¥Î°†*', '^Ïù¥Î°†^'] to the additional_special_tokens key of the tokenizer\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "loss_fn = FocalLoss(gamma=0.5)\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False) :\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "dataset = pd.read_csv(PORORO_TRAIN_PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "models = []\n",
    "stf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed_everything(42))\n",
    "for fold, (train_idx, dev_idx) in enumerate(stf.split(dataset, list(dataset['label']))) :\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    model_config = AutoConfig.from_pretrained(CFG.model_name)\n",
    "    model_config.num_labels = 30\n",
    "\n",
    "    model = CustomModel()\n",
    "    model.to(device)\n",
    "\n",
    "    # Ï∂îÍ∞ÄÌïú token Í∞úÏàòÎßåÌÅº token embedding size ÎäòÎ†§Ï£ºÍ∏∞\n",
    "    # model.resize_token_embeddings(model.tokenizer.vocab_size + added_token_num)\n",
    "\n",
    "    train_dataset = dataset.iloc[train_idx]\n",
    "    dev_dataset = dataset.iloc[dev_idx]\n",
    "\n",
    "    train_label = label_to_num(train_dataset['label'].values)\n",
    "    dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "    tokenized_train = tokenized_dataset(train_dataset, model.tokenizer)\n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, model.tokenizer)\n",
    "\n",
    "    RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=RE_train_dataset,         # training dataset\n",
    "    eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,         # define metrics function\n",
    "    # optimizers = (optimizer, scheduler),\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    # save model\n",
    "    makedirs(f'./best_model/lstm/fold_{fold}/')\n",
    "    model.save_pretrained(f'./best_model/lstm/fold_{fold}/')\n",
    "    \n",
    "    # Prevent OOM error\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /opt/ml/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /opt/ml/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /opt/ml/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.1e08907f1658efd26357385b59d5a6567fc8faf443082618493ad3a2a1a45f0f\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Assigning ['*Î¨ºÎüâ*', '^Î¨ºÎüâ^', '*ÏÇ¨Îûå*', '^ÏÇ¨Îûå^', '*ÏûÑÍ∏∞*', '^ÏûÑÍ∏∞^', '*ÏóÜÏùå*', '^ÏóÜÏùå^', '*Ïù¥Î≤§Ìä∏*', '^Ïù¥Î≤§Ìä∏^', '*Ïä§ÌÑ∞ÎîîÌïÑÎìú*', '^Ïä§ÌÑ∞ÎîîÌïÑÎìú^', '*Ïû¨Î£å*', '^Ïû¨Î£å^', '*ÎèÑÏãú*', '^ÎèÑÏãú^', '*ÏãúÍ∞Ñ*', '^ÏãúÍ∞Ñ^', '*ÎèôÎ¨º*', '^ÎèôÎ¨º^', '*ÏúÑÏπò*', '^ÏúÑÏπò^', '*ÏßàÎ≥ë*', '^ÏßàÎ≥ë^', '*Î¨∏Î™Ö*', '^Î¨∏Î™Ö^', '*ÏßÅÏóÖ*', '^ÏßÅÏóÖ^', '*Ï°∞ÏßÅ*', '^Ï°∞ÏßÅ^', '*ÎÇòÎùº*', '^ÎÇòÎùº^', '*Ïú†Î¨º*', '^Ïú†Î¨º^', '*ÎÇ†Ïßú*', '^ÎÇ†Ïßú^', '*ÏãùÎ¨º*', '^ÏãùÎ¨º^', '*Ïù¥Î°†*', '^Ïù¥Î°†^'] to the additional_special_tokens key of the tokenizer\n",
      "***** Running training *****\n",
      "  Num examples = 25976\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2436\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "<ipython-input-41-92b0c0f817b9>:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='2436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 437/2436 06:06 < 28:04, 1.19 it/s, Epoch 0.54/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}